<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/Phoenix-Logo-16B.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/Phoenix-Logo-16B.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"phoenixdai.cn","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本博客为在2023.07.01-2023.08.01期间阅读的20余篇论文笔记。 主题为：联邦学习、异构性、异步联邦学习、分层联邦学习 文章大体上来自于A、B会和A刊，少数几篇与GLOBECOM相似的出处不限。">
<meta property="og:type" content="article">
<meta property="og:title" content="2023 Summer Vacation Paper">
<meta property="og:url" content="http://phoenixdai.cn/2023/07/09/2023-Summer-Vacation-Paper/index.html">
<meta property="og:site_name" content="Sycamore">
<meta property="og:description" content="本博客为在2023.07.01-2023.08.01期间阅读的20余篇论文笔记。 主题为：联邦学习、异构性、异步联邦学习、分层联邦学习 文章大体上来自于A、B会和A刊，少数几篇与GLOBECOM相似的出处不限。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091442638.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091445675.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091449111.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091506783.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091552130.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091552043.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091557971.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091928203.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307101548971.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307101611546.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307101614108.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307161531550.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307171055108.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307171130478.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307171507557.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307171537350.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307181026935.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307181948523.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307181953470.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307181956792.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307182026462.png">
<meta property="article:published_time" content="2023-07-09T03:31:04.000Z">
<meta property="article:modified_time" content="2023-07-18T12:54:19.940Z">
<meta property="article:author" content="PhoenixDai">
<meta property="article:tag" content="Top-Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091442638.png">

<link rel="canonical" href="http://phoenixdai.cn/2023/07/09/2023-Summer-Vacation-Paper/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>2023 Summer Vacation Paper | Sycamore</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Sycamore</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Phoenix reborns from the ashe</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://phoenixdai.cn/2023/07/09/2023-Summer-Vacation-Paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Phoenix-Logo-White.png">
      <meta itemprop="name" content="PhoenixDai">
      <meta itemprop="description" content="Anything that doesn't kill me makes me stronger">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sycamore">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2023 Summer Vacation Paper
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-09 11:31:04" itemprop="dateCreated datePublished" datetime="2023-07-09T11:31:04+08:00">2023-07-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-07-18 20:54:19" itemprop="dateModified" datetime="2023-07-18T20:54:19+08:00">2023-07-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/XF-TANK/" itemprop="url" rel="index"><span itemprop="name">XF-TANK</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本博客为在2023.07.01-2023.08.01期间阅读的20余篇论文笔记。</p>
<p><strong><em>主题为：联邦学习、异构性、异步联邦学习、分层联邦学习</em></strong></p>
<p>文章大体上来自于A、B会和A刊，少数几篇与GLOBECOM相似的出处不限。</p>
<p><span id="more"></span></p>
<h2 id="asynchronous-hierarchical-federated-learningskim">Asynchronous Hierarchical Federated Learning(skim)</h2>
<blockquote>
<p>Wang, Xing, and Yijun Wang. "Asynchronous hierarchical federated learning." <em>arXiv preprint arXiv:2206.00054</em> (2022).</p>
</blockquote>
<p>本文提出了一种<strong>异步</strong>的<strong>分层</strong>联邦学习，创新点为：1. 利用网络拓扑或聚类算法进行分组；2. 采用异步联邦学习容忍异构性；</p>
<p>👀主要解决<strong>同步分层联邦学习速度慢</strong>。</p>
<p>在联邦学习聚合的时候，每一个群组的leader会有一个队列，这个队列会存储来自每个客户机的参数，周期性地将结果聚合，不用等待一些掉队者的结果。并且作者提出，模型参数越过时，误差越大。因此文章<strong>采用了一个过时函数来控制每个客户机参数的影响程度</strong>。 <span class="math display">\[
\alpha_{t&#39;}=\alpha \times \sigma(t&#39;-t)
\]</span> 这里<span class="math inline">\(\sigma\)</span>是一个单调递减的函数用来控制影响程度。<span class="math inline">\(t&#39;\)</span>为聚合器收到中央服务器下发的最新模型的时间，那些尚未没有被聚合，仍然在队列中的模型参数的时间戳为<span class="math inline">\(t\)</span>，所以<span class="math inline">\(t&#39;&gt;t\)</span>是一定成立的，而<span class="math inline">\(t\)</span>越小，说明这个模型参数越过时，所以<span class="math inline">\(\sigma\)</span>来降低其权重。</p>
<p>在中央服务器上，也有一个队列来存储聚合器的更新，由于异步学习，那么每个聚合器接收到的客户机的更新个数也不一样，所以聚合器上传要将聚合次数<span class="math inline">\(n_k\)</span>、模型参数和时间戳<span class="math inline">\(t&#39;\)</span>一起上传。文章假设最新的全局模型的时间戳为<span class="math inline">\(t&#39;&#39;\)</span>,<span class="math inline">\(t&#39;&#39; &gt; t&#39;\)</span>。和上一层一样，中心聚合器也利用<span class="math inline">\(\sigma\)</span>来权衡每个聚合器的权重，不同的是，聚合器接受的更新个数越多，也会相应提升其贡献。 <span class="math display">\[
\alpha_{t&#39;&#39;}=\frac{n_k}{N}\sigma(t&#39;&#39;-t&#39;)\alpha
\]</span> 其中<span class="math inline">\(N\)</span>为所有的客户机的总数。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091442638.png" alt="image-20230709144148540" style="zoom:110%;" /></p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091445675.png" alt="image-20230709144512623" style="zoom:100%;" /></p>
<p>从结果来看，分层会大大增加学习系统的复杂性，使其收敛速度慢并且不稳定（橙色）。但是文章提出的异步操作缓解了分层的速度慢和稳定性差的问题。并且随着设备数的不断上升，文章的方法（红色）的优势更加明显。</p>
<p>不过另一层面：文章还统计了通信次数，也可以看出来分层其实是很显著地减少<strong>中心聚合器的负担</strong>。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091449111.png" alt="image-20230709144956065" style="zoom:67%;" /></p>
<p>对于本文的未来研究方向，有几个有趣的方向可以追求<strong>。首先，正如作者在论文中提到的，作者的加权机制偏向于对于计算和通信速度更快的设备进行学习，这在数据在客户端上是独立同分布（i.i.d.）的情况下效果很好。如果涉及到非独立同分布（non-i.i.d.）的数据，作者需要设计一个更复杂的加权机制来适应异步联邦学习。</strong>第二个有趣的研究方向是修改本地客户端学习中的简单L2正则化方法。作者还需要完成理论分析的推导和证明。</p>
<h2 id="wscc-a-weight-similarity-based-client-clustering-approach-for-non-iid-federated-learningskim">WSCC: A weight-similarity-based client clustering approach for non-IID federated learning(skim)</h2>
<blockquote>
<p>Tian, Pu, et al. "WSCC: A weight-similarity-based client clustering approach for non-IID federated learning." IEEE Internet of Things Journal 9.20 (2022): 20243-20256.</p>
</blockquote>
<p>本文提出了一种基于权重相似度的<strong>非iid</strong>联邦学习客户端<strong>聚类</strong>方法，创新点：1. 利用余弦距离度量来确定客户端集群；2. 不产生格外数据的情况下优化了通信开销。</p>
<p>👀主要就是为了解决<strong>non-iid的问题</strong></p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091506783.png" alt="image-20230709150621729" style="zoom:100%;" /></p>
<p>非IID类型：</p>
<ol type="1">
<li>标签分布：在这种情况下，节点之间的标签比例并不均匀分布。例如，一个节点可能具有某些标签的较高比例，而其他节点上可能没有出现某些标签。</li>
<li>特征不平衡：这指的是具有相同标签的样本具有不同特征，或者反过来。例如，在手写识别任务中，同一个数字可能有不同来源采集的各种特征。</li>
<li>数据集规模差异：当不同传感器收集的数据量大小不平等时，就会出现这种情况。样本较少的节点容易受到影响。</li>
</ol>
<p>目前的问题是在联邦学习开始时候，各个节点的初始DNN模型是相同的，经过训练之后，如果仍然利用加权平均聚合，没有办法反应非IID的实际全局权重。</p>
<p>因此在非IID情况下，不能把目标函数定义为最小化加权平均的损失函数。而是应该保证每个节点都找到最小化的损失函数。 <span class="math display">\[
w_i^*=argmin_{w_i}\{F_i(w_i)\}
\]</span> 为了解决这个问题，<strong>文章根据数据集的分布将一个联邦学习任务分解成多个同时进行的联邦学习任务，每个集群内部仍然使用IID模型，并用FedAvg协作。</strong></p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091552130.png" alt="image-20230709155220077" style="zoom:90%;" /> <img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091552043.png" alt="image-20230709155241994" style="zoom:90%;" /></p>
<p><strong>在客户端层面：</strong>每个节点的初始模型参数是相同的，但是由于其各自的数据都是非IID的，所以在训练的时候，梯度也会发散，就如同下图一样。每个不同颜色的箭头代表一个分布。相似分布的梯度方向很可能会接近收敛。因此，在<strong>这项工作中，作者将联邦学习训练中的非独立同分布问题视为通过它们的分布对客户端进行聚类，并在同一聚类中聚合权重。</strong></p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091557971.png" alt="image-20230709155740910" style="zoom:100%;" /></p>
<p><em>这里使用余弦相似性的原因为：余弦相似性不受到缩放效应的影响，特别是对于DNN模型的高维权重向量。</em></p>
<p>在每一轮聚合中，作者随机选择一个接收到的参数作为基准，用于计算余弦距离。然后使用距离向量作为后续聚类过程的输入（第5行）</p>
<p>这里文章中使用了AP聚类算法，具体可以见<kbd>ChatGPT</kbd>。对于非独立同分布物联网联邦学习任务，自适应聚类（AP）方法是理想的解决方案，因为它可以自动确定聚类数量。对于存在不确定节点的联邦学习任务，节点分布是未知的。</p>
<p><strong>在节点层面：</strong></p>
<p>这里有一个验证的阶段，节点将会对下发的全局模型进行验证，如果这个模型的准确性与节点内的本地模型的准确性低的程度大于阈值，就放弃该全局模型，继续使用原模型计算。</p>
<h2 id="learn-from-others-and-be-yourself-in-heterogeneous-federated-learning">Learn from Others and Be Yourself in Heterogeneous Federated Learning</h2>
<blockquote>
<p>Huang, Wenke, Mang Ye, and Bo Du. "Learn from others and be yourself in heterogeneous federated learning." <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2022.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/WenkeHuang/FCCL">FCCL github源代码</a></p>
</blockquote>
<p>本文提出了一种联邦互相关和连续学习FCCL（Federated Cross-Correlation and Continual Learning），创新点：1. 通过利用未标记的公共数据和自我监督学习来实现异构模型的可泛化表示；2. 通过使用更新后的模型和预训练模型进行跨领域和内部领域的知识蒸馏，平衡来自其他模型和自身模型的知识。</p>
<p>👀本文解决的主要问题：1. 怎么在异构的联邦学习中学习到一种泛化表示，可以减轻域漂移的问题；2. 怎么平衡多种知识来减少灾难性的遗忘（在本地训练的时候过拟合当前知识而忘记之前的知识），在域间或域内都能表现较好的性能。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307091928203.png" alt="image-20230709192850136" style="zoom:100%;" /></p>
<h3 id="方法">方法</h3>
<p>这个文章总体上提出了两种不同的学习架构：<strong>联邦交叉相关学习（Federated Cross-Correlation Learning）、联邦持续学习（Federated Continual Learning）</strong></p>
<h4 id="联邦交叉相关学习">联邦交叉相关学习</h4>
<p>这个方法受到了自监督学习的启发，自监督学习可以获得一个具有普适性的表示，这对于作者希望得到的可泛化表示是有利的。但是在这个工作中，由于异构的数据，导致相同标签的特征是有明显差异的，所以对于模型来说需<strong>要鼓励相同维度的不变性和不同维度的多样性。</strong>同时不同的模型承载不同的数据，这个数据是具有隐私性的，不适合自监督学习，为了保证安全，文章利用了无标签的公共数据。</p>
<p>通过在不同维度上促进相同维度的不变性和不同维度的多样性，作者可以学习到一个在领域转移下具有普适性的表示。这样的表示能够尽可能地保留关于图像的信息，并在不同领域中具有一定的不变性。</p>
<p>文章构建了一种交叉相关矩阵：对于每个模型的logit输出为<span class="math inline">\(Z_i\)</span>，计算第<span class="math inline">\(i\)</span>个参与者的交叉相关矩阵： <span class="math display">\[
M_i^{uv}=\frac{\sum_b||Z_i^{b,u}||\space||\overline{Z}^{b,v}||}{\sqrt{\sum_b||Z_i^{b,u}||^2}\sqrt{\sum_b||\overline{Z}^{b,v}||^2}}
\]</span> 其中<span class="math inline">\(b\)</span>是批处理样本，<span class="math inline">\(u,v\)</span>均表示logits层的维度索引。<span class="math inline">\(||·||\)</span>是沿着批维度的归一化操作。最终得到<span class="math inline">\(M_i\)</span>是一个维度为<span class="math inline">\(C\)</span>的方阵，其中数据介于-1到1（不相似到相似）。协作的loss值设置为 <span class="math display">\[
L_i^{Col}=\sum_{u}(1-M^{uu})^2+\lambda_{Col}\sum_{u}\sum_{v\neq u}(1+M_{i}^{uv})^2
\]</span> <span class="math inline">\(\lambda_{Col}\)</span>用来权衡两项的重要性，当交叉相关矩阵的对角线元素取值+1时，它鼓励来自不同参与者的logits输出相似；当交叉相关矩阵的非对角线元素取值-1时，它鼓励logits输出的多样性，因为这些logits输出的不同维度将彼此不相关。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307101548971.png" alt="image-20230710154829891" style="zoom:67%;" /></p>
<h4 id="联邦持续学习">联邦持续学习</h4>
<p>传统的监督损失函数会引发两个问题：<strong>1. 在局部更新中，由于没有其他参与者的监督，模型容易过拟合当前数据分布；2. 目标设计通常只根据先验概率独立地对预测结果进行惩罚，这提供了有限且较为僵硬的领域内信息。</strong></p>
<p>因此文章中提出了一种兼顾领域内和领域间的知识蒸馏方法。</p>
<p>在每次中央服务器下发模型之后，每个参与者会获得一个<span class="math inline">\(\theta_i^{t-1}\)</span>，这个模型是通过全局聚合得到的，所以拥有其他参与者学到的知识。</p>
<p>那么组间的知识蒸馏loss就为 <span class="math display">\[
L_i^{Inter}=\sigma(Z_{i,pvt}^{t-1})log\frac{\sigma(Z_{i,pvt}^{t-1})}{\sigma(Z_{i,pvt}^{t,im})}
\]</span> 目的是在保持隐私的同时，不断向他人学习，来保证域间的性能，缓解联邦学习中的灾难性遗忘。</p>
<p>此外对于第<span class="math inline">\(i\)</span>个参与者，作者可以使其在自己的私有数据上对模型进行预训练获得<span class="math inline">\(\theta_i^*\)</span>，然后利用其的logit输出<span class="math inline">\(Z_{i,pvt}^*\)</span>来对域内进行知识蒸馏： <span class="math display">\[
L_i^{Intra}=\sigma(Z_{i,pvt}^*)log\frac{\sigma(Z_{i,pvt}^*)}{\sigma(Z_{i,pvt}^{t,im})}
\]</span> 在一定程度上，上述的两个模型分别代表了域间的老师模型和域内的老师模型（<span class="math inline">\(\theta_i^{t-1}\)</span>，<span class="math inline">\(\theta_i^*\)</span>）</p>
<p>所以最后双向知识蒸馏的loss为两者相加。 <span class="math display">\[
L_i^{Dual}=L_i^{Inter}+L_i^{Intra}
\]</span> 联邦持续学习的整体loss为： <span class="math display">\[
L_i^{Loc}=L_i^{CE}+\lambda_{Loc}L_i^{Dual}
\]</span> <span class="math inline">\(L_i^{CE}\)</span>为传统的交叉熵损失。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307101611546.png" alt="image-20230710161140453" style="zoom:100%;" /></p>
<p>利用双向知识蒸馏学习到的特征在领域内和领域间都更加紧凑且分离。模型展现出更好的区分特征，产生了有前景的领域内和领域间性能。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307101614108.png" alt="image-20230710161456999"  /></p>
<h2 id="feddc-federated-learning-with-non-iid-data-via-local-drift-decoupling-and-correction">FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction</h2>
<blockquote>
<p>Gao, Liang, et al. "Feddc: Federated learning with non-iid data via local drift decoupling and correction." <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2022.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/gaoliang13/FedDC">FedDC github源代码</a></p>
</blockquote>
<p>作者提出了一种新的局部漂移解耦与修正的联邦学习算法(FedDC)，创新点为：1. 动态更新每个客户端的局部目标函数（1. 约束惩罚项：表示全局参数、飘逸变量和局部参数之间的关系；2. 梯度修正项：减少每一轮训练中的梯度漂移）；2. 在训练过程中通过引入漂移变量对局部模型和全局模型进行解耦，减小了局部漂移对全局目标的影响，使其收敛速度更快，达到更好的性能。</p>
<p>👀本文解决的主要问题就是<strong>联邦学习中的客户端漂移问题</strong></p>
<p>在异构的联邦学习中，客户端的局部最优点和全局最优点是不一致的。</p>
<p><strong>客户端漂移</strong></p>
<p>每个客户端在本地数据集上训练的本地模型和直接在全局数据集上训练的全局模型之间存在漂移，如果忽略漂移，服务器将无法得到较好的模型，而非IID数据是高度倾斜的，所以普通的FedAvg性能会明显降低。</p>
<p>举个简单的例子：</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307161531550.png" alt="image-20230716153135403" style="zoom:120%;" /></p>
<p>假设模型中存在一个<strong><em>非线性的</em></strong>变换函数<span class="math inline">\(f\)</span>，<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_2\)</span>是两个客户端的局部参数，<span class="math inline">\(w_c\)</span>为理想的模型参数，<span class="math inline">\(w_f\)</span>是FedAvg获得的参数。客户端的漂移用<span class="math inline">\(h\)</span>表示，那么<span class="math inline">\(h_1 = w_c-\theta_1\)</span>，<span class="math inline">\(h_2=w_c-\theta_2\)</span>。<span class="math inline">\(x\)</span>是一个数据点，那么客户端1对应的输出应该是<span class="math inline">\(y_1=f(\theta_1, x)\)</span>，同样的客户端2的输出为：<span class="math inline">\(y_2=f(\theta_2, x)\)</span>。利用FedAvg获得的模型参数<span class="math inline">\(w_f=\frac{\theta_1+\theta_2}{2}\)</span>，而理想的模型参数应该为<span class="math inline">\(w_c=f^{-1}(\frac{y_1+y_2}{2})/x\)</span>，因为<span class="math inline">\(f\)</span>不是一个线性函数，所以<span class="math inline">\(w_c\neq w_f\)</span>，<span class="math inline">\(f(w_f,x)\neq\frac{y_1+y_2}{2}\)</span>。</p>
<p>这就说明FedAvg的全局模型是漂移的，因此本文章要<strong>学习全局和局部模型之间的局部漂移，并在将局部模型参数上传到服务器之间将局部漂移桥接起来。</strong></p>
<h3 id="方法-1">方法</h3>
<h4 id="优化目标与参数更新">优化目标与参数更新</h4>
<p>首先，文章为每个客户端定义了一个局部漂移变量<span class="math inline">\(h_i\)</span>，在理想的情况下，局部漂移变量应该为<span class="math inline">\(h_i=w-\theta_i\)</span>，这里<span class="math inline">\(w\)</span>为全局模型参数，<span class="math inline">\(\theta_i\)</span>是客户端<span class="math inline">\(i\)</span>的局部模型参数。在训练过程中，需要保持这个限制，所以作者将其作为惩罚项 <span class="math display">\[
R_i(\theta_i, h_i,w)=||h_i+\theta_i-w||^2
\]</span> 通过引入<span class="math inline">\(R\)</span>，作者将方程约束优化，变成无约束优化问题。</p>
<p>最后的目标函数为 <span class="math display">\[
F(\theta_i,h_i,D_i,w)=L_i(\theta_i)+\frac{\alpha}{2}R_i(\theta_i, h_i,w)+G_i(\theta_i,g_i,g)
\]</span> 其中，<span class="math inline">\(L\)</span>是经验损失函数，<span class="math inline">\(G\)</span>是梯度修正项（<span class="math inline">\(G_i(\theta_i,g_i,g)=\frac{1}{\eta K}&lt;\theta_1,g_i-g&gt;\)</span>，其中<span class="math inline">\(\eta\)</span>是学习率，<span class="math inline">\(K\)</span>是一轮训练的迭代次数，<span class="math inline">\(g_i\)</span>是客户端i上一轮本地参数的更新值，<span class="math inline">\(g\)</span>是所有客户端上一轮本地参数的平均更新值。在第t轮，<span class="math inline">\(g_i=\theta_i^t-\theta_i^{t-1},g=E_{i\in[N]}g_i\)</span>），其作用是减少局部梯度的方差。 <span class="math display">\[
\theta_i^{t,k+1}=\theta_i^{t,k}-\eta\frac{\partial F(\theta_i^{t,k},h_i^t,D_i,w^t)}{\partial \theta_i^{t,k}}
\]</span> 👆🏻这个是在第t轮的第k个局部训练迭代中，局部模型参数更新方程，这个方程会在一轮中运行K次。</p>
<p>对于<strong>局部漂移参数的更新</strong>，如果假设局部参数不变，那么更新公式应该是<span class="math inline">\(h_i^+=h_i+(w^+-w)\)</span>，这里的<span class="math inline">\(+\)</span>表示新参数。但是由于全局数据不可用，不可能直接获得全局的最新参数。</p>
<p>因此作者利用<span class="math inline">\(\theta_i\)</span>来代替<span class="math inline">\(w\)</span>，这是因为在每轮开始，都会有<span class="math inline">\(\theta_i=w\)</span>；利用<span class="math inline">\(\theta_i^+\)</span>代替<span class="math inline">\(w^+\)</span>，因为<span class="math inline">\(\theta_i^+\)</span>是对<span class="math inline">\(w^+\)</span>的估计。 <span class="math display">\[
h_i^+=h_i+(w^+-w) \approx h_i+(\theta^+_i-\theta_i)
\]</span> 那么局部模型的参数更新就可以将局部漂移参数引入来减少漂移问题，即<span class="math inline">\(\theta_i^++h_i^+\)</span>，所以聚合方程变成了： <span class="math display">\[
w^+=\sum_{i=1}^N=\frac{D_i}{D}(\theta_i^++h_i^+)
\]</span></p>
<h2 id="scaffold-stochastic-controlled-averaging-for-federated-learning">SCAFFOLD: Stochastic Controlled Averaging for Federated Learning（🧐）</h2>
<blockquote>
<p>Karimireddy, Sai Praneeth, et al. "Scaffold: Stochastic controlled averaging for federated learning." <em>International conference on machine learning</em>. PMLR, 2020.</p>
</blockquote>
<p>文章提出了一种新的随机控制平均算法（SCAFFOLD）。创新点：1. 作者提出了一种新的随机控制平均算法(SCAFFOLD)来通过获得漂移量纠正客户端漂移。作者证明了SCAFFOLD至少和SGD一样快，并且可以收敛于任意异构的数据；2. 作者展示了SCAFFOLD还可以利用客户端之间的相似性来进一步减少所需的通信，这首次证明了采用本地步骤比大批量SGD的优势。3. 作者证明了SCAFFOLD相对不受客户端抽样的影响，从而使得它特别适合于联邦学习。</p>
<p>除此之外😁，文章还证明了：1. SCAFFOLD相对不受客户端抽样的影响，从而使得它特别适合于联邦学习；2. 作者给出了匹配的下界，以证明即使在没有客户端采样和全批处理梯度的情况下，由于客户端漂移，FEDAVG也可以比SGD慢。</p>
<p>👀解决的问题还是<strong>联邦学习中不同客户端的数据漂移</strong>。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307171055108.png" alt="image-20230717105541970" style="zoom:100%;" /></p>
<h3 id="证明部分">证明部分</h3>
<h4 id="fedavg的聚合">FedAvg的聚合</h4>
<p>这个部分分为两个步骤：1. 本地更新模型；2. 聚合客户端的模型来更新服务器模型。</p>
<p>在每轮中，对客户端进行抽样，并对抽样中的每个客户端复制当前的服务器模型<span class="math inline">\(y_i=x\)</span>，并在<span class="math inline">\(K\)</span>次本地更新中执行：<span class="math inline">\(y_i=y_i-\eta_lg_i(y_i)\)</span>，在运行结束后，进行全局聚合：<span class="math inline">\(x=x+\frac{\eta_g}{|S|}\sum_{i\in S}(y_i-x)\)</span>。</p>
<p>这里面<span class="math inline">\(\eta_l\)</span>是局部步长，<span class="math inline">\(\eta_g\)</span>是全局步长。</p>
<p>文章将问题形式化为：最小化随机函数的总和 <span class="math display">\[
min_{x\in R^d}\{f(x)=\frac{1}{N}\sum_{i=1}^{N}(f_i(x)=E_{\zeta_i}[f_i(x;\zeta_i)]\}
\]</span> 这里<span class="math inline">\(f_i()\)</span>表示客户端的损失函数。文章假设<span class="math inline">\(f\)</span>的下界是<span class="math inline">\(f^*\)</span>，<span class="math inline">\(f_i\)</span>是<span class="math inline">\(\beta\)</span>-光滑的，进一步文章假设<span class="math inline">\(g_i(x)=\nabla f_i(x;\zeta_i)\)</span>是<span class="math inline">\(f_i\)</span>的无偏差随机梯度，方差以<span class="math inline">\(\sigma^2\)</span>为界。</p>
<p>对于满足上述条件的FedAvg，其输出在以下三种情况下均小于<span class="math inline">\(\epsilon\)</span>，<span class="math inline">\(R\)</span>具有以下界限：</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307171130478.png" alt="image-20230717113005336" style="zoom:100%;" /></p>
<p>速率改善的原因主要是<strong>使用了两种不同的步长，通过更大的全局步长和较小的局部步长，可以减少客户端漂移。</strong></p>
<h4 id="异构性影响">异构性影响</h4>
<p>本文也对客户端漂移对全局模型影响进行了说明。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307171507557.png" alt="image-20230717150754419" style="zoom:100%;" /></p>
<p>如图所示：</p>
<ol type="1">
<li>一开始服务器下发给客户端的模型是<span class="math inline">\(x\)</span></li>
<li>不同的客户端进行自己的训练，获得了<span class="math inline">\(y_1\)</span>和<span class="math inline">\(y_2\)</span>，并利用FedAvg获得了灰色的全局模型</li>
<li>经过不断的训练，客户端1和客户端2的模型不断分散更新，FedAvg获得的模型为灰黑色圆圈</li>
<li>但是全局最优模型<span class="math inline">\(x^*\)</span>是图中的方块，我们发现普通的平均聚合会导致全局模型与最优模型相差越来越多。</li>
</ol>
<h3 id="算法部分">算法部分</h3>
<p>在算法开始的时候，我们定义两个变量：<span class="math inline">\(c\)</span>：服务器控制变量；<span class="math inline">\(c_i\)</span>：客户端控制变量。</p>
<p>这两个变量的初始化可以均设置为<span class="math inline">\(0\)</span>，但是注意要保证<span class="math inline">\(c=\frac{1}{N}\sum c_i\)</span>，在每一轮的通信中，服务器参数<span class="math inline">\((x,c)\)</span>被传输到了参与客户端<span class="math inline">\(S\subset [N]\)</span>。</p>
<p>在最初始，每个客户端模型会复制服务器模型：<span class="math inline">\(y_i=x\)</span>，然后他在其本地数据上进行训练，执行K轮更新： <span class="math display">\[
y_i=y_i-\eta_l(g_i(y_i)+c-c_i)
\]</span> 然后，本地服务器控制变量将会得到更新 <span class="math display">\[
c_i^+=\left\{  
             \begin{array}{**lr**}  
             Option 1.\quad g_i(x), or &amp;  \\  
             Option 2.\quad c_i-c+\frac{1}{K\eta_l}(x-y_i).
             \end{array}  
\right.
\]</span></p>
<p>第一种选择是计算服务器模型x处的梯度；第二个选择是重新使用先前计算的梯度来控制变量。第二种更👍🏻！</p>
<p>服务器层的更新方程： <span class="math display">\[
x=x+\frac{\eta_g}{|S|}\sum_{i\in S}(y_i-x)
\]</span></p>
<p><span class="math display">\[
c=c+\frac{1}{N}\sum_{i\in S}(c_i^+-c_i)
\]</span></p>
<p>如果<span class="math inline">\(c_i\)</span>总是被设置为0，那么则是传统的FedAvg。</p>
<p>如果不考虑通信成本，那么客户机上的理想更新应该为 <span class="math display">\[
y_i=y_i+\frac{1}{N}\sum_jg_j(y_i)
\]</span> 文章模型利用控制变量获得了 <span class="math display">\[
c_j\approx g_j(y_i)\quad and\quad c\approx \frac{1}{N}\sum_j g_j(y_i)
\]</span> 因此： <span class="math display">\[
(g_i(y_i)-c_i+c)\approx \frac{1}{N}\sum_j g_j(y_i)
\]</span> <img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307171537350.png" alt="image-20230717153717189" style="zoom:100%;" /></p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307181026935.png" alt="image-20230718102637720" style="zoom:50%;" /></p>
<h2 id="addressing-heterogeneity-in-federated-learning-via-distributional-transformation">Addressing Heterogeneity in Federated Learning via Distributional Transformation</h2>
<blockquote>
<p>Yuan, Haolin, et al. "Addressing heterogeneity in federated learning via distributional transformation." <em>European Conference on Computer Vision</em>. Cham: Springer Nature Switzerland, 2022.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/hyhmia/DisTrans">DisTrans github源代码</a></p>
</blockquote>
<p>文章提出了联邦学习分布式转换框架，称为DisTrans。创新点：1. 其不仅仅通过漂移量更改模型，与此同时还通过漂移量更改数据；2. 设计了双通道的神经网络结构；2. 提出了<span class="math inline">\(DH\)</span>异构量化，并利用神经网络进行聚合。</p>
<p>👀解决的问题是<strong>联邦学习中的客户端漂移</strong></p>
<h3 id="动机">动机</h3>
<p>利用漂移量转换每个客户端的训练和测试数据，来提高异构数据下的联邦学习，为了证明其可行性，文章利用了两个例子。</p>
<p><strong>对于非凸训练模型</strong></p>
<p>假设<span class="math inline">\(f(x)=cos(wx)\)</span>，对于每个客户端，通过本地数据<span class="math inline">\(x\)</span>获得<span class="math inline">\(y\)</span>：<span class="math inline">\(y=cos(w_{client_k}^{true}x)+\epsilon_{clinet_k}\)</span>，其中<span class="math inline">\(x\)</span>是从高斯分布中绘制的IID数据，<span class="math inline">\(\epsilon_{clinet_k}\)</span>是均值为0的高斯噪声，漂移量为<span class="math inline">\(px+q\)</span>，<span class="math inline">\(p\)</span>是连个客户端处的固定值，<span class="math inline">\(q\)</span>是通过暴力搜索来选择的。图(a)显示了有漂移的模型更加一致。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307181948523.png" alt="image-20230718194856349" style="zoom:100%;" /></p>
<p><strong>聚合服务器的线性回归问题</strong></p>
<p>作者训练了两个局部线性模型，<span class="math inline">\(f(x)=wx\)</span>，在服务器上进行聚合，重复联邦学习步骤直到收敛。本地训练数据是异构的，生成<span class="math inline">\(y=w^{true}_{clinet_k}x+\epsilon_{clinet_k}\)</span>。当存在漂移时，聚合模型收敛。</p>
<h3 id="方法-2">方法</h3>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307181953470.png" alt="image-20230718195331301" style="zoom:100%;" /></p>
<p>在每一轮中，每个客户端学习一个本地模型和一个漂移量，并将其发送给服务器。服务器聚合客户端的本地模型和漂移量，并将其送回客户端。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307181956792.png" alt="image-20230718195614617" style="zoom:70%;" /></p>
<p>对于本地、全局模型，DisTrans使用了<strong>双输入通道神经体系结构</strong>，如上图所示</p>
<p>文章的神经网络架构有一个共享的backbone，一个密集层链接两个通道的输出，一个logit层合并输出。这两个通道使用相同的漂移量<span class="math inline">\(t\)</span>，以两种方式移动本地数据分布： <span class="math display">\[
x_t=((1-\alpha)x+\alpha t,\quad(1+\alpha)x-\alpha t)
\]</span> 在优化中，每个客户端的目标是实现以下两个目标：</p>
<ol type="1">
<li>优化漂移量，使局部数据更符合局部模型</li>
<li>优化局部模型以适应便宜的局部数据分布</li>
</ol>
<p><span style="background-color: yellow;">这和别的文章不一样，本文章是数据和模型同时漂移。</span></p>
<p>将这两个目标合一获得以下目标方程： <span class="math display">\[
min_{\theta_i^r,t_i^r}L_i^r=\frac{1}{N}\sum_{z_t\in D_{ti}}l(\theta_i^r,z_t)
\]</span> 其中<span class="math inline">\(\theta_i^r\)</span>为客户端<span class="math inline">\(i\)</span>的模型，<span class="math inline">\(t_i^r\)</span>是客户端<span class="math inline">\(i\)</span>的漂移量，<span class="math inline">\(L\)</span>是客户端的损失函数（交叉熵损失），<span class="math inline">\(z_t=(x_t,y)\)</span>是训练样本用漂移量进行漂移，<span class="math inline">\(D_{ti}\)</span>是对于客户端<span class="math inline">\(i\)</span>漂移后的局部训练数据集。</p>
<p>获得<span class="math inline">\(\theta\)</span>是为了解决Goal 2，获得<span class="math inline">\(t\)</span>是为了解决Goal 1。</p>
<p><strong>聚合</strong></p>
<p>文章采用了基于度量的偏移量聚合方式，其提出了一种对异构性的度量方式： <span class="math display">\[
DH=1-\frac{\sum_{j\in [1,N]}c_j}{N\times C}
\]</span> <span class="math inline">\(N\)</span>是所有的类别，<span class="math inline">\(C\)</span>是客户端数量，<span class="math inline">\(c_j\)</span>的定义如下： <span class="math display">\[
c_j=\left\{  
             \begin{array}{**lr**}  
             0,\quad if\space only\space one \space client\space has\space data\space from\space class\space j\\  
             k,\quad if\space k&gt;1\space clients\space have\space data\space from\space class\space j
             \end{array}  
\right.
\]</span> <span class="math inline">\(DH\)</span>是一个<span class="math inline">\([0\%,100\%]\)</span>的量，DisTrans在聚合的时候，如果分布异构性非常大，一个客户端的漂移量可能无法为另一个客户机的漂移量提供信息，因为他们俩数据分布有本质区别，因此文章只会在异构性小于<span class="math inline">\(50\%\)</span>时聚合客户的偏移量。</p>
<p><img src="https://raw.githubusercontent.com/ZimingDai/Picture/main/img/202307182026462.png" alt="image-20230718202636252" style="zoom:67%;" /></p>
<p>假设联邦学习系统的异构性小于阈值。聚合客户端漂移量的一种简单方法是将其平均值作为全局偏移量计算，然后将其发送回所有客户端。然而，这种朴素的聚合方法对所有的客户端使用相同的全局漂移量，从而达到了<strong>次优精度</strong>。</p>
<p>因此，文章提出了基于神经网络的聚合方法。</p>
<p>其将客户端特定的嵌入向量<span class="math inline">\(e\in R^{1\times N}\)</span>和客户端偏移量作为输入，并为客户端输出一个聚合的偏移量：其中嵌入向量的条目<span class="math inline">\(e_i\)</span>就是客户端对类别<span class="math inline">\(i\)</span>的训练数据的训练分数。服务器在训练过程中将聚合作为一个回归问题来学习。在每一轮中，服务器收取一组<span class="math inline">\((t_i,t&#39;_i)\)</span>，其中<span class="math inline">\(t_i\)</span>是当前轮钟来自客户端<span class="math inline">\(i\)</span>的漂移量，<span class="math inline">\(t&#39;_i\)</span>是上一轮的漂移量，服务器利用SGD来最小化两者的距离。</p>
<h2 id="fine-tuning-global-model-via-data-free-knowledge-distillation-for-non-iid-federated-learning">Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning</h2>
<blockquote>
<p>Zhang, Lin, et al. "Fine-tuning global model via data-free knowledge distillation for non-iid federated learning." <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2022.</p>
</blockquote>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>PhoenixDai
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://phoenixdai.cn/2023/07/09/2023-Summer-Vacation-Paper/" title="2023 Summer Vacation Paper">http://phoenixdai.cn/2023/07/09/2023-Summer-Vacation-Paper/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Top-Papers/" rel="tag"># Top-Papers</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/27/Mechanism-of-curiosity/" rel="prev" title="Mechanism of curiosity">
      <i class="fa fa-chevron-left"></i> Mechanism of curiosity
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  
  


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#asynchronous-hierarchical-federated-learningskim"><span class="nav-number">1.</span> <span class="nav-text">Asynchronous Hierarchical Federated Learning(skim)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wscc-a-weight-similarity-based-client-clustering-approach-for-non-iid-federated-learningskim"><span class="nav-number">2.</span> <span class="nav-text">WSCC: A weight-similarity-based client clustering approach for non-IID federated learning(skim)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learn-from-others-and-be-yourself-in-heterogeneous-federated-learning"><span class="nav-number">3.</span> <span class="nav-text">Learn from Others and Be Yourself in Heterogeneous Federated Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%81%94%E9%82%A6%E4%BA%A4%E5%8F%89%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.1.1.</span> <span class="nav-text">联邦交叉相关学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%81%94%E9%82%A6%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.1.2.</span> <span class="nav-text">联邦持续学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#feddc-federated-learning-with-non-iid-data-via-local-drift-decoupling-and-correction"><span class="nav-number">4.</span> <span class="nav-text">FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95-1"><span class="nav-number">4.1.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87%E4%B8%8E%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="nav-number">4.1.1.</span> <span class="nav-text">优化目标与参数更新</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scaffold-stochastic-controlled-averaging-for-federated-learning"><span class="nav-number">5.</span> <span class="nav-text">SCAFFOLD: Stochastic Controlled Averaging for Federated Learning（🧐）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%81%E6%98%8E%E9%83%A8%E5%88%86"><span class="nav-number">5.1.</span> <span class="nav-text">证明部分</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#fedavg%E7%9A%84%E8%81%9A%E5%90%88"><span class="nav-number">5.1.1.</span> <span class="nav-text">FedAvg的聚合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E6%9E%84%E6%80%A7%E5%BD%B1%E5%93%8D"><span class="nav-number">5.1.2.</span> <span class="nav-text">异构性影响</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E9%83%A8%E5%88%86"><span class="nav-number">5.2.</span> <span class="nav-text">算法部分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#addressing-heterogeneity-in-federated-learning-via-distributional-transformation"><span class="nav-number">6.</span> <span class="nav-text">Addressing Heterogeneity in Federated Learning via Distributional Transformation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA"><span class="nav-number">6.1.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95-2"><span class="nav-number">6.2.</span> <span class="nav-text">方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fine-tuning-global-model-via-data-free-knowledge-distillation-for-non-iid-federated-learning"><span class="nav-number">7.</span> <span class="nav-text">Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="PhoenixDai"
      src="/images/Phoenix-Logo-White.png">
  <p class="site-author-name" itemprop="name">PhoenixDai</p>
  <div class="site-description" itemprop="description">Anything that doesn't kill me makes me stronger</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ZimingDai" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ZimingDai" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:phoenixdai2001@163.com" title="E-Mail → mailto:phoenixdai2001@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022-01 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PhoenixDai</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>


        








      </div>
    </footer>
  </div>

  
  
  <script color='48,0,65' opacity='0.9' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
